{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, GridSearchCV, cross_val_score, cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from time import localtime, strftime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Importing the dataset\n",
    "dataset_csv = pd.read_csv('dataset_before_preprocessing.csv', sep=',')\n",
    "\n",
    "# Selecting independent variables\n",
    "features = ['code_churn_avg','contributors_count','hunks_count',\n",
    "            'issue_tracker_issues','cbo','wmc','dit','lcom','max_nested_blocks','total_refactorings',\n",
    "            'duplicated_lines_cpd_density','comment_lines_cloc_density']\n",
    "\n",
    "# Project names\n",
    "# projects = ['arduino','arthas','azkaban','cayenne','deltaspike',\n",
    "#             'exoplayer','fop','gson','javacv','jclouds','joda-time',\n",
    "#             'libgdx','maven','mina','nacos','opennlp','openrefine',\n",
    "#             'pdfbox','redisson','RxJava','testng','vassonic','wss4j',\n",
    "#             'xxl-job','zaproxy']\n",
    "\n",
    "projects = ['arduino','arthas','azkaban','cayenne','deltaspike',\n",
    "            'exoplayer','fop','jclouds','joda-time',\n",
    "            'libgdx','maven','mina','nacos','opennlp','openrefine',\n",
    "            'pdfbox','redisson','RxJava','testng','wss4j',\n",
    "            'xxl-job','zaproxy']\n",
    " \n",
    "# projects = ['arduino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09643cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create f2 scorer\n",
    "f2_scorer = metrics.make_scorer(metrics.fbeta_score, beta=2)\n",
    "\n",
    "# create class_inspection scorer\n",
    "def my_class_inspection(Y_test, y_pred):\n",
    "    cm = metrics.confusion_matrix(Y_test, y_pred)\n",
    "    TN = cm[0][0]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    class_inspection = (TP + FP)/(TN + FN + TP + FP)\n",
    "    return class_inspection\n",
    "class_inspection_scorer = metrics.make_scorer(my_class_inspection)\n",
    "\n",
    "# create class_inspection_reduction scorer\n",
    "def my_class_inspection_reduction(Y_test, y_pred):\n",
    "    cm = metrics.confusion_matrix(Y_test, y_pred)\n",
    "    TN = cm[0][0]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    FP = cm[0][1]\n",
    "    class_inspection = (TP + FP)/(TN + FN + TP + FP)\n",
    "    recall = metrics.recall_score(Y_test, y_pred, average='binary')\n",
    "    class_inspection_reduction = (recall - class_inspection)/recall\n",
    "    return class_inspection_reduction\n",
    "class_inspection_reduction_scorer = metrics.make_scorer(my_class_inspection_reduction)\n",
    "\n",
    "# initialize results dataframe and dict\n",
    "results_df = pd.DataFrame()\n",
    "results = dict()\n",
    "\n",
    "# Get a list of models to evaluate\n",
    "def get_models(X, Y):\n",
    "    models = dict()\n",
    "    \n",
    "    # Logistic Regression\n",
    "    model = LogisticRegression(class_weight='balanced', random_state=0)\n",
    "    parameters = [{'solver': ['lbfgs'],\n",
    "                   'penalty': ['l2', 'none'],\n",
    "                   'C': [0.01, 0.1, 1, 10, 100, 1000]},\n",
    "                  {'solver': ['newton-cg'],\n",
    "                   'penalty': ['l2', 'none'],\n",
    "                   'C': [0.01, 0.1, 1, 10, 100, 1000]},\n",
    "                  {'solver': ['liblinear'],\n",
    "                   'penalty': ['l1', 'l2'],\n",
    "                   'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "    models['lr'] = {'ini_model': model, 'parameters': parameters}\n",
    "    # SVM\n",
    "    model = SVC(class_weight='balanced', max_iter=1000, random_state=0)\n",
    "    parameters = {'kernel': ['linear', 'rbf'],\n",
    "                  'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "    models['svm'] = {'ini_model': model, 'parameters': parameters}\n",
    "    # Random Forest\n",
    "    model = RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "    parameters = {'criterion': ['gini', 'entropy'],\n",
    "                  'max_depth': [1, 2, 4, None],\n",
    "                  'min_samples_leaf': [1, 2, 4],\n",
    "                  'min_samples_split': [2, 5, 10],\n",
    "                  'n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "    models['rf'] = {'ini_model': model, 'parameters': parameters}\n",
    "    # XGBoost\n",
    "    weight_ratio = float(len(Y[Y == 0]))/float(len(Y[Y == 1]))\n",
    "    model = XGBClassifier(scale_pos_weight=weight_ratio, random_state=0)\n",
    "    parameters = {'n_estimators': [10, 20, 50, 100, 200, 400, 600, 800, 1000],\n",
    "                  'max_depth': [1, 2, 5, 10, None]}\n",
    "    models['xgb'] = {'ini_model': model, 'parameters': parameters}\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Tune and evaluate a given model using Stratified cross-validation and Grid Search to find optimal hyperparameters and scores \n",
    "def tune_model(model, parameters, X, Y):\n",
    "    scoring = {'accuracy':'accuracy',\n",
    "               'precision':'precision',\n",
    "               'recall':'recall',\n",
    "               'f1':'f1',\n",
    "               'f2':f2_scorer,\n",
    "               'class_inspection':class_inspection_scorer,\n",
    "               'class_inspection_reduction':class_inspection_reduction_scorer}\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=scoring, cv=cv, refit='f2', n_jobs=-1, verbose = 1)\n",
    "    tuned_model = grid_search.fit(X, Y)\n",
    "    \n",
    "    return tuned_model\n",
    "\n",
    "# for each project\n",
    "for project in projects:\n",
    "    print('======== Starting experiments for %s project ========' % project)\n",
    "    \n",
    "    # fetch rows for a specific project\n",
    "    dataset = dataset_csv.loc[dataset_csv['project_name'] == project]\n",
    "    \n",
    "    # create X and Y\n",
    "    X = dataset[features]\n",
    "    Y = dataset['Max-Ruler']\n",
    "    \n",
    "    # get the models to evaluate\n",
    "    models = get_models(X, Y)\n",
    "    \n",
    "    # evaluate the models and store results\n",
    "    results[project] = {}\n",
    "    temp_results_df = pd.DataFrame()\n",
    "    for name, model in models.items():\n",
    "        print('==== Evaluating %s (%s) =====' % (name, strftime('%d-%m-%Y %H:%M:%S', localtime())))\n",
    "        \n",
    "        print('- running Grid Search')\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # execute Grid Search and get the tuned model\n",
    "        tuned_model = tune_model(model['ini_model'], model['parameters'], X, Y)\n",
    "        \n",
    "        print('- grid search finished in %.4f s' % (time.time() - t0))\n",
    "        print('- best parameters: %s' % tuned_model.best_params_)\n",
    "        print('- best performance: %s' % round(tuned_model.best_score_, 3))\n",
    "        \n",
    "        # fill results dict\n",
    "        results[project][name] = {}\n",
    "        results[project][name]['Best Parameters'] = tuned_model.best_params_\n",
    "        results[project][name]['Best Index'] = tuned_model.best_index_\n",
    "        results[project][name]['Performance'] = tuned_model.cv_results_\n",
    "        \n",
    "        # fill results dataframe\n",
    "        temp_results_df = temp_results_df.append({\n",
    "            'Project': project,\n",
    "            'Model': name,\n",
    "            'Best Parameters': tuned_model.best_params_,\n",
    "            'Accuracy':'%.3f (%.3f)' % (tuned_model.cv_results_['mean_test_accuracy'][tuned_model.best_index_], tuned_model.cv_results_['std_test_accuracy'][tuned_model.best_index_]),\n",
    "            'Precision':'%.3f (%.3f)' % (tuned_model.cv_results_['mean_test_precision'][tuned_model.best_index_], tuned_model.cv_results_['std_test_precision'][tuned_model.best_index_]),\n",
    "            'Recall':'%.3f (%.3f)' % (tuned_model.cv_results_['mean_test_recall'][tuned_model.best_index_], tuned_model.cv_results_['std_test_recall'][tuned_model.best_index_]),\n",
    "            'F1-score':'%.3f (%.3f)' % (tuned_model.cv_results_['mean_test_f1'][tuned_model.best_index_], tuned_model.cv_results_['std_test_f1'][tuned_model.best_index_]),\n",
    "            'F2-score':'%.3f (%.3f)' % (tuned_model.cv_results_['mean_test_f2'][tuned_model.best_index_], tuned_model.cv_results_['std_test_f2'][tuned_model.best_index_]),\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    results_df = results_df.append(temp_results_df, ignore_index=True)\n",
    "\n",
    "# print results dataframe\n",
    "results_df.set_index('Project', inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a673630",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key_project, value_project in results.items():\n",
    "#     print(key_project)\n",
    "#     for key_model, value_model in value_project.items():\n",
    "#         print(key_model)\n",
    "        \n",
    "# plot model performance for comparison\n",
    "acc = list()\n",
    "pre = list()\n",
    "rec = list()\n",
    "f1 = list()\n",
    "f2 = list()\n",
    "ci = list()\n",
    "\n",
    "for name in results:\n",
    "    acc.append(results[name]['test_accuracy'].tolist())\n",
    "    pre.append(results[name]['test_precision'].tolist())\n",
    "    rec.append(results[name]['test_recall'].tolist())\n",
    "    f1.append(results[name]['test_f1'].tolist())\n",
    "    f2.append(results[name]['test_f2'].tolist())\n",
    "    ci.append(results[name]['test_class_inspection'].tolist())\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(16, 14))\n",
    "# axs[0, 0].set_title('accuracy')\n",
    "# axs[0, 0].boxplot(acc, labels=list(results.keys()), showmeans=True)\n",
    "axs[0, 0].set_title('precision')\n",
    "axs[0, 0].boxplot(pre, labels=list(results.keys()), showmeans=True)\n",
    "axs[0, 1].set_title('recall')\n",
    "axs[0, 1].boxplot(rec, labels=list(results.keys()), showmeans=True)\n",
    "axs[1, 0].set_title('F1')\n",
    "axs[1, 0].boxplot(f1, labels=list(results.keys()), showmeans=True)\n",
    "axs[1, 1].set_title('F2')\n",
    "axs[1, 1].boxplot(f2, labels=list(results.keys()), showmeans=True)\n",
    "axs[2, 0].set_title('Class Inspection')\n",
    "axs[2, 0].boxplot(ci, labels=list(results.keys()), showmeans=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9118e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create f2 scorer\n",
    "# f2_scorer = metrics.make_scorer(metrics.fbeta_score, beta=2)\n",
    "\n",
    "# # create class_inspection scorer\n",
    "# def my_class_inspection(Y_test, y_pred):\n",
    "#     cm = metrics.confusion_matrix(Y_test, y_pred)\n",
    "#     TN = cm[0][0]\n",
    "#     FN = cm[1][0]\n",
    "#     TP = cm[1][1]\n",
    "#     FP = cm[0][1]\n",
    "#     class_inspection = (TP + FP)/(TN + FN + TP + FP)\n",
    "#     return class_inspection\n",
    "# class_inspection_scorer = metrics.make_scorer(my_class_inspection)\n",
    "\n",
    "# # create class_inspection_reduction scorer\n",
    "# def my_class_inspection_reduction(Y_test, y_pred):\n",
    "#     cm = metrics.confusion_matrix(Y_test, y_pred)\n",
    "#     TN = cm[0][0]\n",
    "#     FN = cm[1][0]\n",
    "#     TP = cm[1][1]\n",
    "#     FP = cm[0][1]\n",
    "#     class_inspection = (TP + FP)/(TN + FN + TP + FP)\n",
    "#     recall = metrics.recall_score(Y_test, y_pred, average='binary')\n",
    "#     class_inspection_reduction = (recall - class_inspection)/recall\n",
    "#     return class_inspection_reduction\n",
    "# class_inspection_reduction_scorer = metrics.make_scorer(my_class_inspection_reduction)\n",
    "\n",
    "# # initialize results dataframe and dict\n",
    "# results_df = pd.DataFrame()\n",
    "# results = dict()\n",
    "\n",
    "# # Get a list of models to evaluate\n",
    "# def get_models(X, Y):\n",
    "#     models = dict()\n",
    "    \n",
    "#     # Logistic Regression\n",
    "#     model = LogisticRegression(class_weight='balanced', random_state=0)\n",
    "#     parameters = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#                   'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "#     models['lr'] = {'ini_model': model, 'parameters': parameters}\n",
    "# #     # SVM\n",
    "# #     model = SVC(class_weight='balanced', random_state=0)\n",
    "# #     parameters = {'kernel': ['linear', 'rbf'],\n",
    "# #                   'C': [0.01, 0.1, 1, 10, 100, 1000]}\n",
    "# #     models['svm'] = {'ini_model': model, 'parameters': parameters}\n",
    "# #     # Random Forest\n",
    "# #     model = RandomForestClassifier(class_weight='balanced', random_state=0)\n",
    "# #     parameters = {'model__criterion': ['gini', 'entropy'],\n",
    "# #                   'model__max_depth': [1, 2, 4, 6, 8, 10, None],\n",
    "# #                   'model__min_samples_leaf': [1, 2, 4],\n",
    "# #                   'model__min_samples_split': [2, 5, 10],\n",
    "# #                   'model__n_estimators': [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    "# #     models['rf'] = {'ini_model': model, 'parameters': parameters}\n",
    "# #     # XGBoost\n",
    "# #     weight_ratio = float(len(Y[Y == 0]))/float(len(Y[Y == 1]))\n",
    "# #     model = XGBClassifier(scale_pos_weight=weight_ratio, random_state=0)\n",
    "# #     parameters = {'model__n_estimators': [5, 10, 50, 100, 200, 400, 600, 800, 1000],\n",
    "# #                   'model__max_depth': [None, 2, 5, 10]}\n",
    "# #     models['xgb'] = {'ini_model': model, 'parameters': parameters}\n",
    "    \n",
    "#     return models\n",
    "\n",
    "# # Tune a given model using Grid Search to find optimal hyperparameters\n",
    "# def tune_model(model, parameters, X, Y):\n",
    "#     scoring = {'accuracy':'accuracy',\n",
    "#                'precision':'precision',\n",
    "#                'recall':'recall',\n",
    "#                'f1':'f1',\n",
    "#                'f2':f2_scorer,\n",
    "#                'class_inspection':class_inspection_scorer,\n",
    "#                'class_inspection_reduction':class_inspection_reduction_scorer}\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "#     grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring=f2_scorer, cv=cv, refit=True, n_jobs=-1, verbose = 5)\n",
    "#     tuned_model = grid_search.fit(X, Y)\n",
    "#     best_accuracy = tuned_model.best_score_\n",
    "#     best_parameters = tuned_model.best_params_\n",
    "    \n",
    "#     return tuned_model, best_accuracy, best_parameters\n",
    "\n",
    "# # Evaluate a given model using Stratified cross-validation\n",
    "# def evaluate_model(model, X, Y):\n",
    "#     scoring = {'accuracy':'accuracy',\n",
    "#                'precision':'precision',\n",
    "#                'recall':'recall',\n",
    "#                'f1':'f1',\n",
    "#                'f2':f2_scorer,\n",
    "#                'class_inspection':class_inspection_scorer,\n",
    "#                'class_inspection_reduction':class_inspection_reduction_scorer}\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "#     scores = cross_validate(model, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "    \n",
    "#     return scores\n",
    "\n",
    "# # for each project\n",
    "# for project in projects:\n",
    "#     print('======== Starting experiments for %s project ========' % project)\n",
    "    \n",
    "#     # fetch rows for a specific project\n",
    "#     dataset = dataset_csv.loc[dataset_csv['project_name'] == project]\n",
    "    \n",
    "#     # create X and Y\n",
    "#     X = dataset[features]\n",
    "#     Y = dataset['Max-Ruler']\n",
    "    \n",
    "#     # get the models to evaluate\n",
    "#     models = get_models(X, Y)\n",
    "    \n",
    "#     # evaluate the models and store results\n",
    "#     results[project] = {}\n",
    "#     temp_results_df = pd.DataFrame()\n",
    "#     for name, model in models.items():\n",
    "#         print('==== Evaluating %s =====' % name)\n",
    "        \n",
    "#         # execute Grid Search and get the tuned model\n",
    "#         print('- running Grid Search')\n",
    "#         tuned_model, best_accuracy, best_parameters = tune_model(model['ini_model'], model['parameters'], X, Y)\n",
    "#         # re-evaluate the tuned model using Stratified cross-validation\n",
    "#         print('- running stratified cross-validation')\n",
    "#         scores = evaluate_model(tuned_model, X, Y)\n",
    "        \n",
    "#         # fill results dict\n",
    "#         results[project][name] = {}\n",
    "#         results[project][name]['Best Parameters'] = best_parameters\n",
    "#         results[project][name]['Performance'] = scores\n",
    "        \n",
    "#         # fill results dataframe\n",
    "#         temp_results_df = temp_results_df.append({\n",
    "#             'Project': project,\n",
    "#             'Model': name,\n",
    "#             'Best Parameters': best_parameters,\n",
    "#             'Accuracy':'%.3f (%.3f)' % (scores['test_accuracy'].mean(), scores['test_accuracy'].std()),\n",
    "#             'Precision':'%.3f (%.3f)' % (scores['test_precision'].mean(), scores['test_precision'].std()), \n",
    "#             'Recall':'%.3f (%.3f)' % (scores['test_recall'].mean(), scores['test_recall'].std()),\n",
    "#             'F1-score':'%.3f (%.3f)' % (scores['test_f1'].mean(), scores['test_f1'].std()),\n",
    "#             'F2-score':'%.3f (%.3f)' % (scores['test_f2'].mean(), scores['test_f2'].std()),\n",
    "#         }, ignore_index=True)\n",
    "\n",
    "#     results_df = results_df.append(temp_results_df, ignore_index=True)\n",
    "\n",
    "# # print results dataframe\n",
    "# results_df.set_index('Project', inplace=True)\n",
    "# results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
